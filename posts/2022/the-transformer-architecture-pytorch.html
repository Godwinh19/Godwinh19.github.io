
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Architecture du transformer et impl√©mentation avec Pytorch (Partie I) &#8212; Godwin H</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../_static/nav.css?v=9d3b4cd0" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'posts/2022/the-transformer-architecture-pytorch';</script>
    <link rel="canonical" href="https://godwinh19.github.io/posts/2022/the-transformer-architecture-pytorch.html" />
    <link rel="icon" href="../../_static/ico.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

<link
  rel="alternate"
  type="application/atom+xml"
  href="../../blog/atom.xml"
  title="All Posts"
/>



  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Godwin H</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../about/index.html">
    About me
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../blog/index.html">
    All Posts
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
        </div>
      
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/godwinh19" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://linkedin.com/in/godwin-houdji" title="LinkedIn" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i>
            <span class="sr-only">LinkedIn</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://twitter.com/GodwinHoudji" title="Twitter" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Twitter</span></a>
        </li>
</ul></div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../about/index.html">
    About me
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../blog/index.html">
    All Posts
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/godwinh19" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://linkedin.com/in/godwin-houdji" title="LinkedIn" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i>
            <span class="sr-only">LinkedIn</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://twitter.com/GodwinHoudji" title="Twitter" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Twitter</span></a>
        </li>
</ul></div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">Architecture...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
<section class="tex2jax_ignore mathjax_ignore" id="architecture-du-transformer-et-implementation-avec-pytorch-partie-i">
<h1>Architecture du transformer et impl√©mentation avec Pytorch (Partie I)<a class="headerlink" href="#architecture-du-transformer-et-implementation-avec-pytorch-partie-i" title="Link to this heading">#</a></h1>
<figure>
<img src="https://www.researchgate.net/profile/Dennis-Gannon-2/publication/339390384/figure/fig1/AS:860759328321536@1582232424168/The-transformer-model-from-Attention-is-all-you-need-Viswani-et-al.jpg" alt="transformer-architecture">
<figcaption align = "center"><b>Fig. Architecture du mod√®le du transformer</b></figcaption>
</figure>
<p><strong>Qu‚Äôest ce qu‚Äôun transformer ?:</strong>
Un mod√®le de transformer est un r√©seau neuronal qui apprend le contexte et donc le sens en
suivant les relations dans les donn√©es s√©quentielles comme les mots de cette phrase.
Dans la version originale de leur papier <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a>, les auteurs d√©finissent le transformer comme une nouvelle architecture de r√©seau simple bas√©e uniquement sur les m√©canismes d‚Äôattention, exempt√©e enti√®rement de r√©currence ou de convolution.</p>
<p><strong>Applications</strong></p>
<p>Les transformers constituent ce qu‚Äôon appelle maintenant la fondation des mod√®les de deep learning. Ils sont utilis√©s entre autres pour les t√¢ches comme:</p>
<ul class="simple">
<li><p>Question r√©ponse</p></li>
<li><p>Classification de texte</p></li>
<li><p>Extraction d‚Äôinformation</p></li>
<li><p>Reconnaissance d‚Äôobjets</p></li>
<li><p>Analyse de sentiment</p></li>
<li><p>L√©gende des images</p></li>
<li><p>etc</p></li>
</ul>
<p>Le transformer est constitu√© de deux grandes parties que sont l‚Äôencodeur et le d√©codeur.
<strong>L‚Äôencodeur</strong> prend la s√©quence d‚Äôentr√©e et cr√©e une repr√©sentation contextuelle (√©galement appel√©e contexte) de celle-ci. Le <strong>d√©codeur</strong> prend cette repr√©sentation contextuelle en entr√©e et g√©n√®re la s√©quence de sortie. Le processus peut √™tre r√©sum√© comme suit:</p>
<p><img alt="process" src="https://latex.codecogs.com/svg.image?%5Cmathbf%7BX%7D&amp;space;%5CLongrightarrow&amp;space;%5Cmathbf%7BEncodeur%7D&amp;space;%5CLongrightarrow&amp;space;%5Cmathbf%7Bcontexte%7D&amp;space;%5CLongrightarrow&amp;space;%5Cmathbf%7BDecodeur%7D&amp;space;%5CLongrightarrow&amp;space;%5Cmathbf%7BY%7D" /></p>
<p><strong>X</strong> constitue l‚Äôentr√©e et <strong>Y</strong> la sortie.</p>
<p>Dans cette premi√®re partie, nous allons suivre le processus bloc par bloc afin de construire notre propre encoder du tranformer. Commen√ßons par le point le plus important: le <code class="docutils literal notranslate"><span class="pre">self</span> <span class="pre">attention</span></code>.</p>
<section id="self-attention-ou-auto-attention">
<h2>Self-Attention ou Auto-Attention<a class="headerlink" href="#self-attention-ou-auto-attention" title="Link to this heading">#</a></h2>
<p>Il existe plusieurs fa√ßons de mettre en ≈ìuvre une couche d‚Äôauto-attention, mais la plus courante est l‚Äôattention par produit scalaire, tir√©e de l‚Äôarticle pr√©sentant l‚Äôarchitecture du transformer Quatre √©tapes principales sont n√©cessaires pour mettre en ≈ìuvre ce m√©canisme :</p>
<ul class="simple">
<li><p>Projection de chaque encastrement de jeton dans trois vecteurs appel√©s <code class="docutils literal notranslate"><span class="pre">key</span></code>,<code class="docutils literal notranslate"><span class="pre">query</span></code>,<code class="docutils literal notranslate"><span class="pre">value</span></code>.</p></li>
<li><p>Calculer les scores d‚Äôattention. Nous d√©terminons dans quelle mesure les vecteurs de <code class="docutils literal notranslate"><span class="pre">query</span></code> et de <code class="docutils literal notranslate"><span class="pre">key</span></code> sont li√©s les uns aux autres en utilisant une fonction de similarit√©. Comme son nom l‚Äôindique, la fonction de similarit√© pour l‚Äôattention par produit scalaire est le produit scalaire, calcul√© efficacement en utilisant la multiplication matricielle des incorporations. Les <code class="docutils literal notranslate"><span class="pre">query</span></code> et les <code class="docutils literal notranslate"><span class="pre">key</span></code> qui sont similaires auront un produit scalaire important, tandis que ceux qui n‚Äôont pas beaucoup de points communs n‚Äôauront que peu ou pas de chevauchement. Les r√©sultats de cette √©tape sont appel√©s les <strong>scores d‚Äôattention</strong>, et pour une s√©quence de <img alt="math" src="https://latex.codecogs.com/svg.image?n" /> tokens d‚Äôentr√©e, il existe une matrice <img alt="math" src="https://latex.codecogs.com/svg.image?n*n" /> correspondante de scores d‚Äôattention.</p></li>
<li><p>Calculer les poids d‚Äôattention. Les produits scalaires peuvent en g√©n√©ral produire des nombres arbitrairement grands, ce qui peut d√©stabiliser le processus de formation. Pour y rem√©dier, les scores d‚Äôattention sont d‚Äôabord multipli√©s par un facteur d‚Äô√©chelle afin de normaliser leur variance, puis normalis√©s √† l‚Äôaide d‚Äôun softmax afin de s‚Äôassurer que la somme de toutes les valeurs des colonnes est √©gale √† 1. Le r√©sultat des <img alt="nbyn" src="https://latex.codecogs.com/svg.image?n*n" /> matrice contient maintenant tous les poids d‚Äôattention <img alt="math" src="https://latex.codecogs.com/svg.image?w_%7Bji%7D" /></p></li>
<li><p>Mise √† jour de l‚Äôint√©gration des jetons. Une fois les poids d‚Äôattention calcul√©s, nous les multiplions par le vecteur de valeurs (<code class="docutils literal notranslate"><span class="pre">value</span></code>) afin d‚Äôobtenir une repr√©sentation actualis√©e pour l‚Äôincorporation: <img alt="update weights" src="https://latex.codecogs.com/svg.image?x_i'&amp;space;=&amp;space;%5Csum_%7Bj%7Dw_%7Bji%7Dv_j" /></p></li>
</ul>
<p>En premier temps, nous allons extraire les tokens de notre texte:</p>
<p><em><strong>PS:</strong></em> Pour plus de facilit√© nous travaillerons avec les hyper-param√®tres utilis√©s dans l‚Äôarchitecture <strong>BERT</strong>. Retrouvez le model de Bert sur huggingface <a class="reference external" href="https://huggingface.co/bert-base-uncased">ici</a> et le papier <a class="reference external" href="https://arxiv.org/abs/1810.04805">ici</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model_ckpt</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_ckpt</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;time flies like an arrow&quot;</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[ 2051, 10029,  2066,  2019,  8612]])
</pre></div>
</div>
<p>Nous avons ajout√© le <code class="docutils literal notranslate"><span class="pre">add_special_tokens=False</span></code> pour ignorer les tokens sp√©ciales comme [CLS] et [SEP].
Ensuite, nous devons cr√©er des incorporations denses. Dans ce contexte, dense signifie que chaque entr√©e dans les incorporations contient une valeur non nulle. Ces incorporations sont des vecteurs z√©ros avec une seule valeur de 1 √† une position donn√©e: <code class="docutils literal notranslate"><span class="pre">one</span> <span class="pre">hot</span> <span class="pre">encoding</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_ckpt</span><span class="p">)</span>
<span class="n">config</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>BertConfig {
  &quot;architectures&quot;: [
    &quot;BertForMaskedLM&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.1,
  &quot;classifier_dropout&quot;: null,
  &quot;gradient_checkpointing&quot;: false,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.1,
  &quot;hidden_size&quot;: 768,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;max_position_embeddings&quot;: 512,
  &quot;model_type&quot;: &quot;bert&quot;,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_layers&quot;: 12,
  &quot;pad_token_id&quot;: 0,
  &quot;position_embedding_type&quot;: &quot;absolute&quot;,
  &quot;transformers_version&quot;: &quot;4.11.3&quot;,
  &quot;type_vocab_size&quot;: 2,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 30522
}
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">token_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
<span class="n">token_emb</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Embedding(30522, 768)
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">token_emb</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>
<span class="n">inputs_embeds</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 5, 768])
</pre></div>
</div>
<p>Pour l‚Äôinstant, nous allons remettre √† plus tard l‚Äôencodage de position et passer √† la cr√©ation des cl√©s, requ√™tes et valeurs en utilisant le produit scalaire comme fonction de similarit√©.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">math</span><span class="w"> </span><span class="kn">import</span> <span class="n">sqrt</span>

<span class="n">query</span> <span class="o">=</span> <span class="n">key</span> <span class="o">=</span> <span class="n">value</span> <span class="o">=</span> <span class="n">inputs_embeds</span>
<span class="n">dim_k</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">dim_k</span><span class="p">)</span>
<span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 5, 5])
</pre></div>
</div>
<p>La division par <img alt="math" src="https://latex.codecogs.com/svg.image?%5Csqrt%7Bd_k%7D" /> du score permet de ne pas avoir de grandes valeurs durant l‚Äôentra√Ænement.
Par la suite, appliquons la fonction softmax qui va permettre d‚Äôavoir une somme des scores √©gale √† 1.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="n">weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">weights</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[1., 1., 1., 1., 1.]], grad_fn=&lt;SumBackward1&gt;)
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">attn_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
<span class="n">attn_outputs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 5, 768])
</pre></div>
</div>
<p>Nous venons de finir une impl√©mentation simplifi√©e d‚Äôauto-attention. Nous rappelons que tous le processus est juste une multiplication matricielle et une fonction softmax.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="n">dim_k</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">dim_k</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
<p>Le self attention est finalement calcul√© comme suit:</p>
<p><img alt="math" src="https://latex.codecogs.com/svg.image?Attention(Q,K,V)&amp;space;=&amp;space;softmax(%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D)V" /></p>
</section>
<section id="multi-head-attention-attention-a-tetes-multiples">
<h2>Multi-head attention: attention √† t√™tes multiples<a class="headerlink" href="#multi-head-attention-attention-a-tetes-multiples" title="Link to this heading">#</a></h2>
<p>En pratique, la couche d‚Äôauto-attention applique trois transformations lin√©aires ind√©pendantes √† chaque incorporation pour g√©n√©rer les vecteurs de requ√™te, de cl√© et de valeur. Ces transformations projettent les ench√¢ssements et chaque projection porte son propre ensemble de param√®tres apprenables, ce qui permet √† la couche d‚Äôauto-attention de se concentrer sur diff√©rents aspects s√©mantiques de la s√©quence.</p>
<p>Il s‚Äôav√®re √©galement avantageux de disposer de plusieurs ensembles de projections lin√©aires, chacun repr√©sentant une t√™te d‚Äôattention.Mais pourquoi avons-nous besoin de plus d‚Äôune t√™te d‚Äôattention ? La raison est que la softmax d‚Äôune t√™te a tendance √† se concentrer sur un seul aspect de la similarit√©. Le fait d‚Äôavoir plusieurs t√™tes permet au mod√®le de se concentrer sur plusieurs aspects √† la fois. Par exemple, une t√™te peut se concentrer sur l‚Äôinteraction sujet-verbe, tandis qu‚Äôune autre trouve des adjectifs proches. Il est √©vident que nous n‚Äôint√©grons pas ces relations dans le mod√®le et qu‚Äôelles sont enti√®rement apprises √† partir des donn√©es. Une analogie peut √™tre faite avec les mod√®les de vision par ordinateur. Nous avons les filtres des r√©seaux neuronaux convolutifs, o√π un filtre peut √™tre responsable de la d√©tection des visages et un autre de la recherche des roues de voitures dans les images.</p>
<p>Commen√ßons d‚Äôabord par impl√©menter une seule attention √† t√™te:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">AttentionHead</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
        <span class="n">attn_outputs</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attn_outputs</span>
</pre></div>
</div>
<p>En pratique, on choisit la valeur de <code class="docutils literal notranslate"><span class="pre">embed_dim</span></code> pour qu‚Äôelle soit un multiple de <code class="docutils literal notranslate"><span class="pre">head_dim</span></code>. En prenant l‚Äôexemple de l‚Äôarchitecture BERT, la dimension de l‚Äôent√™te est de 768/12 = 64.</p>
<p>Maintenant que nous avons une seule t√™te d‚Äôattention, nous pouvons concat√©ner les sorties de chacune d‚Äôelles pour mettre en ≈ìuvre la couche d‚Äôattention multi-t√™tes compl√®te :</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">AttentionHead</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="c1"># A la sortie de cette couche, nous avons un vecteur [batch, embed_dim, head_dim]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">h</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># la fonction cat permet de concatener les sorties de la couche d&#39;attention √† </span>
        <span class="c1"># seule t√™te pour avoir un vecteur [batch, embed_dim, head_dim*num_heads]</span>
        <span class="c1"># head_dim*num_heads encore √©gal √† embed_dim: entr√©e de la couche lin√©aire suivante.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
    
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">multihead_attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">attn_output</span> <span class="o">=</span> <span class="n">multihead_attn</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="p">)</span>
<span class="n">attn_output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 5, 768])
</pre></div>
</div>
</section>
<section id="the-feed-forward-layer-ou-couche-a-propagation-avant">
<h2>The Feed-Forward Layer ou couche √† propagation avant<a class="headerlink" href="#the-feed-forward-layer-ou-couche-a-propagation-avant" title="Link to this heading">#</a></h2>
<p>Cette sous-couche dans le transformer est un simple r√©seau neuronal enti√®rement connect√© √† deux couches, mais avec une particularit√© : au lieu de traiter la s√©quence enti√®re d‚Äôincorporations comme un vecteur unique, elle traite chaque incorporation ind√©pendamment. C‚Äôest la raison pour laquelle cette couche est souvent appel√©e couche √† propagation avant en fonction de la position.</p>
<p>Une r√®gle empirique tir√©e de la litt√©rature est que le <code class="docutils literal notranslate"><span class="pre">hidden_size</span></code> de la premi√®re couche doit √™tre quatre fois sup√©rieure √† la taille des embeddings, et une fonction d‚Äôactivation <code class="docutils literal notranslate"><span class="pre">GELU</span></code> est le plus souvent utilis√©e.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gelu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">ff_outputs</span> <span class="o">=</span> <span class="n">feed_forward</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
<span class="n">ff_outputs</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 5, 768])
</pre></div>
</div>
<p>Nous avons maintenant tous les ingr√©dients pour cr√©er une couche d‚Äôencodeur du transformer ! La seule d√©cision qui reste √† prendre est de savoir o√π placer les connexions de saut et la normalisation de la couche.</p>
</section>
<section id="layer-normalisation-ou-normalisation-de-couche">
<h2>Layer Normalisation ou normalisation de couche<a class="headerlink" href="#layer-normalisation-ou-normalisation-de-couche" title="Link to this heading">#</a></h2>
<p>Le transformer normalise chaque entr√©e du lot pour qu‚Äôelle ait une moyenne nulle et une variance unitaire. Les connexions de saut passent un tenseur (non trait√©) √† la couche suivante du mod√®le et l‚Äôajoute au tenseur trait√©. Dans la litt√©rature, nous avons deux options possibles pour la normalisation:</p>
<ul class="simple">
<li><p>Normalisation post-couche: ici la normalisation est effectu√©e entre les connexions de saut. Cette disposition est d√©licate √† former √† partir de z√©ro car les gradients peuvent diverger. Pour cette raison, vous verrez souvent un concept connu sous le nom de <code class="docutils literal notranslate"><span class="pre">learning</span> <span class="pre">rate</span> <span class="pre">warm-up</span></code>, o√π le taux d‚Äôapprentissage est progressivement augment√© d‚Äôune petite valeur √† une valeur maximale pendant l‚Äôentra√Ænement.</p></li>
<li><p>Normalisation de la pr√©-couche: Il s‚Äôagit de la disposition la plus courante trouv√©e dans la litt√©rature; elle place la normalisation de couche dans la port√©e des connexions de saut. Elle tend √† √™tre beaucoup plus stable pendant l‚Äôapprentissage et ne n√©cessite g√©n√©ralement pas l‚Äôusage du <code class="docutils literal notranslate"><span class="pre">learning</span> <span class="pre">rate</span> <span class="pre">warm-up</span></code>.</p></li>
</ul>
<p>Nous allons utiliser la seconde option et √©crire l‚Äôencodeur de notre transformer de la fa√ßon suivante:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">TransformerEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l_norm_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l_norm_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># 1- layer normalisation</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l_norm_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># 2- apply attention with skip connection</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="c1"># 3- feed forward layer with skip connection</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l_norm_2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">encoder_layer</span> <span class="o">=</span> <span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">encoder_layer</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>(torch.Size([1, 5, 768]), torch.Size([1, 5, 768]))
</pre></div>
</div>
<p>Actuellement les couches de l‚Äôencodeur sont invariants par rapport √† la position des token. Nous allons ajouter cette nouvelle information en utilisant le <code class="docutils literal notranslate"><span class="pre">positional</span> <span class="pre">embeddings</span></code></p>
</section>
<section id="positional-embeddings-ou-encastrement-positionnels">
<h2>Positional embeddings ou encastrement positionnels<a class="headerlink" href="#positional-embeddings-ou-encastrement-positionnels" title="Link to this heading">#</a></h2>
<p>Son but est de permettre au mod√®le d‚Äôapprendre la formation des tokens. Etant donn√© qu‚Äôune phrase n‚Äôa de sens que si les ordres des mots sont respect√©s. Cette couche apprend cette constitution des mots.</p>
<p>Cr√©ons un module Embeddings personnalis√© qui combine une couche d‚Äôembeddings de tokens qui projette les <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> au <em>hidden state dense</em> avec l‚Äôembedding positionnel qui fait la m√™me chose pour les <code class="docutils literal notranslate"><span class="pre">position_ids</span></code>.
Dans la configuration de <strong>BERT</strong> par exemple, la taille maximale d‚Äôun paragraphe pris en compte est de 512, ce qui veut dire que nous allons fix√© comme position maximale d‚Äôun token √† 512.
L‚Äôincorporation r√©sultante est simplement la somme des deux incorporations :</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Embeddings</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">token_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">()</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">):</span>
        <span class="c1"># Positions ids for the inputs</span>
        <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">positions_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># Now we create position and token embeddings</span>
        <span class="n">token_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_embeddings</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="n">position_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings</span><span class="p">(</span><span class="n">positions_ids</span><span class="p">)</span>
        
        <span class="c1"># token combination</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">token_embeddings</span> <span class="o">+</span> <span class="n">position_embeddings</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">embeddings</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">Embeddings</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">embedding_layer</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 5, 768])
</pre></div>
</div>
<p>Comprendre plus sur le positional embedding <a href="https://www.youtube.com/watch?v=dichIcUZfOw" target="_blank" rel="noopener">ici</a></p>
<p>Combinons ces diff√©rentes √©tapes pour construire la couche d‚Äôencodage.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">Embeddings</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> 
                                     <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)])</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">encoder</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">encoder</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 5, 768])
</pre></div>
</div>
<p>A cette √©tape, nous avons l‚Äô√©tat cach√© de chaque token. Un grand avantage des mod√®les de transformer est qu‚Äôil peuvent √™tre divis√© en deux parties:</p>
<ul class="simple">
<li><p>en un corps ind√©pendant de la t√¢che et</p></li>
<li><p>une t√™te sp√©cifique √† la t√¢che.</p></li>
</ul>
<p>Notre encodeur √©tant pr√™t √† l‚Äôutilisation, ajoutons une t√™te de couche qui sera utilis√© pour la classification.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">TransformerForSequenceClassification</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">encoder_classifier</span> <span class="o">=</span> <span class="n">TransformerForSequenceClassification</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">encoder_classifier</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 3])
</pre></div>
</div>
<p>Nous avons d√©fini pour notre t√¢che de classification <em>3 cat√©gories</em>, les donn√©es sont
envoy√©es vers la sous-couche de classification apr√®s passage de l‚Äôencodeur et du
dropout.</p>
<p>Ceci marque la fin de l‚Äôencodeur !!.</p>
</section>
<section id="endnote">
<h2>Endnote<a class="headerlink" href="#endnote" title="Link to this heading">#</a></h2>
<p>Le notebook est disponible <a href="https://github.com/Godwinh19/ds-portfolio/blob/main/transformer_architecture.ipynb" target="_blank" rel="noopener">ici</a>.
Cheers ‚òï!</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="ressources">
<h1>Ressources<a class="headerlink" href="#ressources" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p>Natural Language Processing with Transformers <a href="https://github.com/nlp-with-transformers/notebooks" target="_blank" rel="noopener">github</a></p></li>
<li><p>Natural Language Processing with Transformers <a href="https://www.oreilly.com/library/view/natural-language-processing/9781098103231/" target="_blank" rel="noopener">the book</a></p></li>
<li><p>Attention is all you need: <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">papier</a></p></li>
<li><p>The annoted transformer, par Guillaume Klein et al, <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">ici</a></p></li>
<li><p>The Illustrated Transformer par Jay Alammar <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">ici</a></p></li>
<li><p><a href="https://medium.com/nerd-for-tech/nlp-theory-and-code-encoder-decoder-models-part-11-30-e686bcb61dc7" target="_blank" rel="noopener">NLP Theory and Code: Encoder-Decoder Models</a></p></li>
</ul>
</section>

<div class="section ablog__blog_comments">
  
  


<div class="section ablog__prev-next">
  <span class="ablog__prev">
    
    
    <a href="bert-classification-for-research-papers.html">
      
      <i class="fa fa-arrow-circle-left"></i>
      
      <span>Bert Classification For Research Papers</span>
    </a>
    
  </span>
  <span class="ablog__spacer">&nbsp;</span>
  <span class="ablog__next">
    
  </span>
</div>

  
  
  <div class="section ablog__comments">
    <h2>Comments</h2>
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname = "disqus_qfyj9nwrcT";
      var disqus_identifier = "/posts/2022/the-transformer-architecture-pytorch/";
      var disqus_title = "Architecture du transformer et impl√©mentation avec Pytorch (Partie I)";
      var disqus_url = "https://godwinh19.github.io/posts/2022/the-transformer-architecture-pytorch";

      (function () {
        var dsq = document.createElement("script");
        dsq.type = "text/javascript";
        dsq.async = true;
        dsq.src = "//" + disqus_shortname + ".disqus.com/embed.js";
        (
          document.getElementsByTagName("head")[0] ||
          document.getElementsByTagName("body")[0]
        ).appendChild(dsq);
      })();
    </script>
    <noscript>
      Please enable JavaScript to view the
      <a href="https://disqus.com/?ref_noscript">
        comments powered by Disqus.</a ></noscript >
    <a href="https://disqus.com" class="dsq-brlink">
      comments powered by <span class="logo-disqus">Disqus</span>
    </a >
  </div>
  
</div>

                </article>
              
              
              
              
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Architecture du transformer et impl√©mentation avec Pytorch (Partie I)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-ou-auto-attention">Self-Attention ou Auto-Attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention-attention-a-tetes-multiples">Multi-head attention: attention √† t√™tes multiples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-feed-forward-layer-ou-couche-a-propagation-avant">The Feed-Forward Layer ou couche √† propagation avant</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalisation-ou-normalisation-de-couche">Layer Normalisation ou normalisation de couche</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-embeddings-ou-encastrement-positionnels">Positional embeddings ou encastrement positionnels</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#endnote">Endnote</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#ressources">Ressources</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p style="text-align:center; font-size:0.9rem; color:#666;">
  ¬© Copyright 2022, Godwin H.
  <br>
  Powered by <a href="https://www.sphinx-doc.org/" target="_blank" rel="noopener">Sphinx</a>
  and the <a href="https://pydata-sphinx-theme.readthedocs.io/" target="_blank" rel="noopener">PyData Sphinx Theme</a>.
  <br>
  <a href="https://github.com/godwinh19" target="_blank">GitHub</a> ¬∑
  <a href="/about">About</a> ¬∑
  <a href="/blog">Blog</a>
</p>

<!--2022 Godwin Houdji ‚Äî Built with ‚ù§Ô∏è, data, and too much ‚òï--></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>