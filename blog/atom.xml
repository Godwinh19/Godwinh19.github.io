<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>https://godwinh19.github.io/</id>
  <title>Godwin H</title>
  <updated>2022-05-27T16:18:26.171557+00:00</updated>
  <link href="https://godwinh19.github.io/"/>
  <link href="https://godwinh19.github.io/blog/atom.xml" rel="self"/>
  <generator uri="https://ablog.readthedocs.org/" version="0.10.23">ABlog</generator>
  <entry>
    <id>https://godwinh19.github.io/posts/2022/bert-classification-for-research-papers/</id>
    <title>Bert Classification For Research Papers</title>
    <updated>2022-03-22T00:00:00+01:00</updated>
    <content type="html">&lt;div class="ablog-post-excerpt docutils container"&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Our goal is to build a model that uses the abstract and title of a paper to predict whether it will be rejected or not.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This is an article with code to refine BERT to perform text classification on a dataset of accepted and rejected scientific papers.&lt;/p&gt;
&lt;/div&gt;
</content>
    <link href="https://godwinh19.github.io/posts/2022/bert-classification-for-research-papers/" rel="alternate"/>
    <summary>Our goal is to build a model that uses the abstract and title of a paper to predict whether it will be rejected or not.This is an article with code to refine BERT to perform text classification on a dataset of accepted and rejected scientific papers.</summary>
    <category term="deeplearning" label="deep learning"/>
    <category term="nlp" label="nlp"/>
    <category term="bert" label="bert"/>
    <category term="transformers" label="transformers"/>
    <published>2022-03-22T00:00:00+01:00</published>
  </entry>
  <entry>
    <id>https://godwinh19.github.io/posts/2022/the-transformer-architecture-pytorch/</id>
    <title>Architecture du transformer et implémentation avec Pytorch (Partie I)</title>
    <updated>2022-05-26T00:00:00+01:00</updated>
    <content type="html">&lt;div class="ablog-post-excerpt docutils container"&gt;
&lt;p&gt;&lt;strong&gt;Qu’est ce qu’un transformer ?:&lt;/strong&gt;
Un modèle de transformer est un réseau neuronal qui apprend le contexte et donc le sens en suivant les relations dans les données séquentielles comme les mots de cette phrase. Dans la version originale de leur papier &lt;a class="reference external" href="https://arxiv.org/abs/1706.03762"&gt;Attention Is All You Need&lt;/a&gt;, les auteurs définisse le transformer comme une nouvelle architecture de réseau simple basée uniquement sur les mécanismes d’attention, exemptée entièrement de récurrence ou de convolution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt;
&lt;img alt="process" src="https://latex.codecogs.com/svg.image?%5Cmathbf%7BX%7D&amp;amp;space%3B%5CLongrightarrow&amp;amp;space%3B%5Cmathbf%7BEncodeur%7D&amp;amp;space%3B%5CLongrightarrow&amp;amp;space%3B%5Cmathbf%7Bcontexte%7D&amp;amp;space%3B%5CLongrightarrow&amp;amp;space%3B%5Cmathbf%7BDecodeur%7D&amp;amp;space%3B%5CLongrightarrow&amp;amp;space%3B%5Cmathbf%7BY%7D" /&gt;
&lt;/div&gt;
</content>
    <link href="https://godwinh19.github.io/posts/2022/the-transformer-architecture-pytorch/" rel="alternate"/>
    <summary>Qu’est ce qu’un transformer ?:
Un modèle de transformer est un réseau neuronal qui apprend le contexte et donc le sens en suivant les relations dans les données séquentielles comme les mots de cette phrase. Dans la version originale de leur papier Attention Is All You Need, les auteurs définisse le transformer comme une nouvelle architecture de réseau simple basée uniquement sur les mécanismes d’attention, exemptée entièrement de récurrence ou de convolution.Applicationsprocess</summary>
    <category term="deeplearning" label="deep learning"/>
    <category term="nlp" label="nlp"/>
    <category term="transformers" label="transformers"/>
    <category term="attention" label="attention"/>
    <published>2022-05-26T00:00:00+01:00</published>
  </entry>
</feed>
