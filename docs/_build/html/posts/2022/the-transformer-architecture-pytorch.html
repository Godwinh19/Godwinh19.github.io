
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Architecture du transformer et implémentation avec Pytorch (Partie I) &#8212; Godwin H</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../_static/nav.css?v=9d3b4cd0" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'posts/2022/the-transformer-architecture-pytorch';</script>
    <link rel="canonical" href="https://godwinh19.github.io/posts/2022/the-transformer-architecture-pytorch.html" />
    <link rel="icon" href="../../_static/ico.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

<link
  rel="alternate"
  type="application/atom+xml"
  href="../../blog/atom.xml"
  title="All Posts"
/>



  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Godwin H</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../about/index.html">
    About me
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../blog/index.html">
    All Posts
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
        </div>
      
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/godwinh19" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://linkedin.com/in/godwin-houdji" title="LinkedIn" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i>
            <span class="sr-only">LinkedIn</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://twitter.com/GodwinHoudji" title="Twitter" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Twitter</span></a>
        </li>
</ul></div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../about/index.html">
    About me
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../blog/index.html">
    All Posts
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/godwinh19" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://linkedin.com/in/godwin-houdji" title="LinkedIn" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i>
            <span class="sr-only">LinkedIn</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://twitter.com/GodwinHoudji" title="Twitter" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Twitter</span></a>
        </li>
</ul></div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">Architecture...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
<section class="tex2jax_ignore mathjax_ignore" id="architecture-du-transformer-et-implementation-avec-pytorch-partie-i">
<h1>Architecture du transformer et implémentation avec Pytorch (Partie I)<a class="headerlink" href="#architecture-du-transformer-et-implementation-avec-pytorch-partie-i" title="Link to this heading">#</a></h1>
<figure>
<img src="https://www.researchgate.net/profile/Dennis-Gannon-2/publication/339390384/figure/fig1/AS:860759328321536@1582232424168/The-transformer-model-from-Attention-is-all-you-need-Viswani-et-al.jpg" alt="transformer-architecture">
<figcaption align = "center"><b>Fig. Architecture du modèle du transformer</b></figcaption>
</figure>
<p><strong>Qu’est ce qu’un transformer ?:</strong>
Un modèle de transformer est un réseau neuronal qui apprend le contexte et donc le sens en
suivant les relations dans les données séquentielles comme les mots de cette phrase.
Dans la version originale de leur papier <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a>, les auteurs définissent le transformer comme une nouvelle architecture de réseau simple basée uniquement sur les mécanismes d’attention, exemptée entièrement de récurrence ou de convolution.</p>
<p><strong>Applications</strong></p>
<p>Les transformers constituent ce qu’on appelle maintenant la fondation des modèles de deep learning. Ils sont utilisés entre autres pour les tâches comme:</p>
<ul class="simple">
<li><p>Question réponse</p></li>
<li><p>Classification de texte</p></li>
<li><p>Extraction d’information</p></li>
<li><p>Reconnaissance d’objets</p></li>
<li><p>Analyse de sentiment</p></li>
<li><p>Légende des images</p></li>
<li><p>etc</p></li>
</ul>
<p>Le transformer est constitué de deux grandes parties que sont l’encodeur et le décodeur.
<strong>L’encodeur</strong> prend la séquence d’entrée et crée une représentation contextuelle (également appelée contexte) de celle-ci. Le <strong>décodeur</strong> prend cette représentation contextuelle en entrée et génère la séquence de sortie. Le processus peut être résumé comme suit:</p>
<p><img alt="process" src="https://latex.codecogs.com/svg.image?%5Cmathbf%7BX%7D&amp;space;%5CLongrightarrow&amp;space;%5Cmathbf%7BEncodeur%7D&amp;space;%5CLongrightarrow&amp;space;%5Cmathbf%7Bcontexte%7D&amp;space;%5CLongrightarrow&amp;space;%5Cmathbf%7BDecodeur%7D&amp;space;%5CLongrightarrow&amp;space;%5Cmathbf%7BY%7D" /></p>
<p><strong>X</strong> constitue l’entrée et <strong>Y</strong> la sortie.</p>
<p>Dans cette première partie, nous allons suivre le processus bloc par bloc afin de construire notre propre encoder du tranformer. Commençons par le point le plus important: le <code class="docutils literal notranslate"><span class="pre">self</span> <span class="pre">attention</span></code>.</p>
<section id="self-attention-ou-auto-attention">
<h2>Self-Attention ou Auto-Attention<a class="headerlink" href="#self-attention-ou-auto-attention" title="Link to this heading">#</a></h2>
<p>Il existe plusieurs façons de mettre en œuvre une couche d’auto-attention, mais la plus courante est l’attention par produit scalaire, tirée de l’article présentant l’architecture du transformer Quatre étapes principales sont nécessaires pour mettre en œuvre ce mécanisme :</p>
<ul class="simple">
<li><p>Projection de chaque encastrement de jeton dans trois vecteurs appelés <code class="docutils literal notranslate"><span class="pre">key</span></code>,<code class="docutils literal notranslate"><span class="pre">query</span></code>,<code class="docutils literal notranslate"><span class="pre">value</span></code>.</p></li>
<li><p>Calculer les scores d’attention. Nous déterminons dans quelle mesure les vecteurs de <code class="docutils literal notranslate"><span class="pre">query</span></code> et de <code class="docutils literal notranslate"><span class="pre">key</span></code> sont liés les uns aux autres en utilisant une fonction de similarité. Comme son nom l’indique, la fonction de similarité pour l’attention par produit scalaire est le produit scalaire, calculé efficacement en utilisant la multiplication matricielle des incorporations. Les <code class="docutils literal notranslate"><span class="pre">query</span></code> et les <code class="docutils literal notranslate"><span class="pre">key</span></code> qui sont similaires auront un produit scalaire important, tandis que ceux qui n’ont pas beaucoup de points communs n’auront que peu ou pas de chevauchement. Les résultats de cette étape sont appelés les <strong>scores d’attention</strong>, et pour une séquence de <img alt="math" src="https://latex.codecogs.com/svg.image?n" /> tokens d’entrée, il existe une matrice <img alt="math" src="https://latex.codecogs.com/svg.image?n*n" /> correspondante de scores d’attention.</p></li>
<li><p>Calculer les poids d’attention. Les produits scalaires peuvent en général produire des nombres arbitrairement grands, ce qui peut déstabiliser le processus de formation. Pour y remédier, les scores d’attention sont d’abord multipliés par un facteur d’échelle afin de normaliser leur variance, puis normalisés à l’aide d’un softmax afin de s’assurer que la somme de toutes les valeurs des colonnes est égale à 1. Le résultat des <img alt="nbyn" src="https://latex.codecogs.com/svg.image?n*n" /> matrice contient maintenant tous les poids d’attention <img alt="math" src="https://latex.codecogs.com/svg.image?w_%7Bji%7D" /></p></li>
<li><p>Mise à jour de l’intégration des jetons. Une fois les poids d’attention calculés, nous les multiplions par le vecteur de valeurs (<code class="docutils literal notranslate"><span class="pre">value</span></code>) afin d’obtenir une représentation actualisée pour l’incorporation: <img alt="update weights" src="https://latex.codecogs.com/svg.image?x_i'&amp;space;=&amp;space;%5Csum_%7Bj%7Dw_%7Bji%7Dv_j" /></p></li>
</ul>
<p>En premier temps, nous allons extraire les tokens de notre texte:</p>
<p><em><strong>PS:</strong></em> Pour plus de facilité nous travaillerons avec les hyper-paramètres utilisés dans l’architecture <strong>BERT</strong>. Retrouvez le model de Bert sur huggingface <a class="reference external" href="https://huggingface.co/bert-base-uncased">ici</a> et le papier <a class="reference external" href="https://arxiv.org/abs/1810.04805">ici</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model_ckpt</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_ckpt</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;time flies like an arrow&quot;</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[ 2051, 10029,  2066,  2019,  8612]])
</pre></div>
</div>
<p>Nous avons ajouté le <code class="docutils literal notranslate"><span class="pre">add_special_tokens=False</span></code> pour ignorer les tokens spéciales comme [CLS] et [SEP].
Ensuite, nous devons créer des incorporations denses. Dans ce contexte, dense signifie que chaque entrée dans les incorporations contient une valeur non nulle. Ces incorporations sont des vecteurs zéros avec une seule valeur de 1 à une position donnée: <code class="docutils literal notranslate"><span class="pre">one</span> <span class="pre">hot</span> <span class="pre">encoding</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_ckpt</span><span class="p">)</span>
<span class="n">config</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>BertConfig {
  &quot;architectures&quot;: [
    &quot;BertForMaskedLM&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.1,
  &quot;classifier_dropout&quot;: null,
  &quot;gradient_checkpointing&quot;: false,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.1,
  &quot;hidden_size&quot;: 768,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;max_position_embeddings&quot;: 512,
  &quot;model_type&quot;: &quot;bert&quot;,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_layers&quot;: 12,
  &quot;pad_token_id&quot;: 0,
  &quot;position_embedding_type&quot;: &quot;absolute&quot;,
  &quot;transformers_version&quot;: &quot;4.11.3&quot;,
  &quot;type_vocab_size&quot;: 2,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 30522
}
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">token_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
<span class="n">token_emb</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Embedding(30522, 768)
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">token_emb</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>
<span class="n">inputs_embeds</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 5, 768])
</pre></div>
</div>
<p>Pour l’instant, nous allons remettre à plus tard l’encodage de position et passer à la création des clés, requêtes et valeurs en utilisant le produit scalaire comme fonction de similarité.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">math</span><span class="w"> </span><span class="kn">import</span> <span class="n">sqrt</span>

<span class="n">query</span> <span class="o">=</span> <span class="n">key</span> <span class="o">=</span> <span class="n">value</span> <span class="o">=</span> <span class="n">inputs_embeds</span>
<span class="n">dim_k</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">dim_k</span><span class="p">)</span>
<span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 5, 5])
</pre></div>
</div>
<p>La division par <img alt="math" src="https://latex.codecogs.com/svg.image?%5Csqrt%7Bd_k%7D" /> du score permet de ne pas avoir de grandes valeurs durant l’entraînement.
Par la suite, appliquons la fonction softmax qui va permettre d’avoir une somme des scores égale à 1.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="n">weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">weights</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[1., 1., 1., 1., 1.]], grad_fn=&lt;SumBackward1&gt;)
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">attn_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
<span class="n">attn_outputs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 5, 768])
</pre></div>
</div>
<p>Nous venons de finir une implémentation simplifiée d’auto-attention. Nous rappelons que tous le processus est juste une multiplication matricielle et une fonction softmax.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="n">dim_k</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">dim_k</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
<p>Le self attention est finalement calculé comme suit:</p>
<p><img alt="math" src="https://latex.codecogs.com/svg.image?Attention(Q,K,V)&amp;space;=&amp;space;softmax(%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D)V" /></p>
</section>
<section id="multi-head-attention-attention-a-tetes-multiples">
<h2>Multi-head attention: attention à têtes multiples<a class="headerlink" href="#multi-head-attention-attention-a-tetes-multiples" title="Link to this heading">#</a></h2>
<p>En pratique, la couche d’auto-attention applique trois transformations linéaires indépendantes à chaque incorporation pour générer les vecteurs de requête, de clé et de valeur. Ces transformations projettent les enchâssements et chaque projection porte son propre ensemble de paramètres apprenables, ce qui permet à la couche d’auto-attention de se concentrer sur différents aspects sémantiques de la séquence.</p>
<p>Il s’avère également avantageux de disposer de plusieurs ensembles de projections linéaires, chacun représentant une tête d’attention.Mais pourquoi avons-nous besoin de plus d’une tête d’attention ? La raison est que la softmax d’une tête a tendance à se concentrer sur un seul aspect de la similarité. Le fait d’avoir plusieurs têtes permet au modèle de se concentrer sur plusieurs aspects à la fois. Par exemple, une tête peut se concentrer sur l’interaction sujet-verbe, tandis qu’une autre trouve des adjectifs proches. Il est évident que nous n’intégrons pas ces relations dans le modèle et qu’elles sont entièrement apprises à partir des données. Une analogie peut être faite avec les modèles de vision par ordinateur. Nous avons les filtres des réseaux neuronaux convolutifs, où un filtre peut être responsable de la détection des visages et un autre de la recherche des roues de voitures dans les images.</p>
<p>Commençons d’abord par implémenter une seule attention à tête:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">AttentionHead</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
        <span class="n">attn_outputs</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attn_outputs</span>
</pre></div>
</div>
<p>En pratique, on choisit la valeur de <code class="docutils literal notranslate"><span class="pre">embed_dim</span></code> pour qu’elle soit un multiple de <code class="docutils literal notranslate"><span class="pre">head_dim</span></code>. En prenant l’exemple de l’architecture BERT, la dimension de l’entête est de 768/12 = 64.</p>
<p>Maintenant que nous avons une seule tête d’attention, nous pouvons concaténer les sorties de chacune d’elles pour mettre en œuvre la couche d’attention multi-têtes complète :</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">AttentionHead</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="c1"># A la sortie de cette couche, nous avons un vecteur [batch, embed_dim, head_dim]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">h</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># la fonction cat permet de concatener les sorties de la couche d&#39;attention à </span>
        <span class="c1"># seule tête pour avoir un vecteur [batch, embed_dim, head_dim*num_heads]</span>
        <span class="c1"># head_dim*num_heads encore égal à embed_dim: entrée de la couche linéaire suivante.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
    
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">multihead_attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">attn_output</span> <span class="o">=</span> <span class="n">multihead_attn</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="p">)</span>
<span class="n">attn_output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 5, 768])
</pre></div>
</div>
</section>
<section id="the-feed-forward-layer-ou-couche-a-propagation-avant">
<h2>The Feed-Forward Layer ou couche à propagation avant<a class="headerlink" href="#the-feed-forward-layer-ou-couche-a-propagation-avant" title="Link to this heading">#</a></h2>
<p>Cette sous-couche dans le transformer est un simple réseau neuronal entièrement connecté à deux couches, mais avec une particularité : au lieu de traiter la séquence entière d’incorporations comme un vecteur unique, elle traite chaque incorporation indépendamment. C’est la raison pour laquelle cette couche est souvent appelée couche à propagation avant en fonction de la position.</p>
<p>Une règle empirique tirée de la littérature est que le <code class="docutils literal notranslate"><span class="pre">hidden_size</span></code> de la première couche doit être quatre fois supérieure à la taille des embeddings, et une fonction d’activation <code class="docutils literal notranslate"><span class="pre">GELU</span></code> est le plus souvent utilisée.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gelu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">ff_outputs</span> <span class="o">=</span> <span class="n">feed_forward</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
<span class="n">ff_outputs</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 5, 768])
</pre></div>
</div>
<p>Nous avons maintenant tous les ingrédients pour créer une couche d’encodeur du transformer ! La seule décision qui reste à prendre est de savoir où placer les connexions de saut et la normalisation de la couche.</p>
</section>
<section id="layer-normalisation-ou-normalisation-de-couche">
<h2>Layer Normalisation ou normalisation de couche<a class="headerlink" href="#layer-normalisation-ou-normalisation-de-couche" title="Link to this heading">#</a></h2>
<p>Le transformer normalise chaque entrée du lot pour qu’elle ait une moyenne nulle et une variance unitaire. Les connexions de saut passent un tenseur (non traité) à la couche suivante du modèle et l’ajoute au tenseur traité. Dans la littérature, nous avons deux options possibles pour la normalisation:</p>
<ul class="simple">
<li><p>Normalisation post-couche: ici la normalisation est effectuée entre les connexions de saut. Cette disposition est délicate à former à partir de zéro car les gradients peuvent diverger. Pour cette raison, vous verrez souvent un concept connu sous le nom de <code class="docutils literal notranslate"><span class="pre">learning</span> <span class="pre">rate</span> <span class="pre">warm-up</span></code>, où le taux d’apprentissage est progressivement augmenté d’une petite valeur à une valeur maximale pendant l’entraînement.</p></li>
<li><p>Normalisation de la pré-couche: Il s’agit de la disposition la plus courante trouvée dans la littérature; elle place la normalisation de couche dans la portée des connexions de saut. Elle tend à être beaucoup plus stable pendant l’apprentissage et ne nécessite généralement pas l’usage du <code class="docutils literal notranslate"><span class="pre">learning</span> <span class="pre">rate</span> <span class="pre">warm-up</span></code>.</p></li>
</ul>
<p>Nous allons utiliser la seconde option et écrire l’encodeur de notre transformer de la façon suivante:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">TransformerEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l_norm_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l_norm_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># 1- layer normalisation</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l_norm_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># 2- apply attention with skip connection</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="c1"># 3- feed forward layer with skip connection</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l_norm_2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">encoder_layer</span> <span class="o">=</span> <span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">encoder_layer</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>(torch.Size([1, 5, 768]), torch.Size([1, 5, 768]))
</pre></div>
</div>
<p>Actuellement les couches de l’encodeur sont invariants par rapport à la position des token. Nous allons ajouter cette nouvelle information en utilisant le <code class="docutils literal notranslate"><span class="pre">positional</span> <span class="pre">embeddings</span></code></p>
</section>
<section id="positional-embeddings-ou-encastrement-positionnels">
<h2>Positional embeddings ou encastrement positionnels<a class="headerlink" href="#positional-embeddings-ou-encastrement-positionnels" title="Link to this heading">#</a></h2>
<p>Son but est de permettre au modèle d’apprendre la formation des tokens. Etant donné qu’une phrase n’a de sens que si les ordres des mots sont respectés. Cette couche apprend cette constitution des mots.</p>
<p>Créons un module Embeddings personnalisé qui combine une couche d’embeddings de tokens qui projette les <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> au <em>hidden state dense</em> avec l’embedding positionnel qui fait la même chose pour les <code class="docutils literal notranslate"><span class="pre">position_ids</span></code>.
Dans la configuration de <strong>BERT</strong> par exemple, la taille maximale d’un paragraphe pris en compte est de 512, ce qui veut dire que nous allons fixé comme position maximale d’un token à 512.
L’incorporation résultante est simplement la somme des deux incorporations :</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Embeddings</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">token_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">()</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">):</span>
        <span class="c1"># Positions ids for the inputs</span>
        <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">positions_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># Now we create position and token embeddings</span>
        <span class="n">token_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_embeddings</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="n">position_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings</span><span class="p">(</span><span class="n">positions_ids</span><span class="p">)</span>
        
        <span class="c1"># token combination</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">token_embeddings</span> <span class="o">+</span> <span class="n">position_embeddings</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">embeddings</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">Embeddings</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">embedding_layer</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 5, 768])
</pre></div>
</div>
<p>Comprendre plus sur le positional embedding <a href="https://www.youtube.com/watch?v=dichIcUZfOw" target="_blank" rel="noopener">ici</a></p>
<p>Combinons ces différentes étapes pour construire la couche d’encodage.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">Embeddings</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> 
                                     <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)])</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">encoder</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">encoder</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 5, 768])
</pre></div>
</div>
<p>A cette étape, nous avons l’état caché de chaque token. Un grand avantage des modèles de transformer est qu’il peuvent être divisé en deux parties:</p>
<ul class="simple">
<li><p>en un corps indépendant de la tâche et</p></li>
<li><p>une tête spécifique à la tâche.</p></li>
</ul>
<p>Notre encodeur étant prêt à l’utilisation, ajoutons une tête de couche qui sera utilisé pour la classification.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">TransformerForSequenceClassification</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">encoder_classifier</span> <span class="o">=</span> <span class="n">TransformerForSequenceClassification</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">encoder_classifier</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 3])
</pre></div>
</div>
<p>Nous avons défini pour notre tâche de classification <em>3 catégories</em>, les données sont
envoyées vers la sous-couche de classification après passage de l’encodeur et du
dropout.</p>
<p>Ceci marque la fin de l’encodeur !!.</p>
</section>
<section id="endnote">
<h2>Endnote<a class="headerlink" href="#endnote" title="Link to this heading">#</a></h2>
<p>Le notebook est disponible <a href="https://github.com/Godwinh19/ds-portfolio/blob/main/transformer_architecture.ipynb" target="_blank" rel="noopener">ici</a>.
Cheers ☕!</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="ressources">
<h1>Ressources<a class="headerlink" href="#ressources" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p>Natural Language Processing with Transformers <a href="https://github.com/nlp-with-transformers/notebooks" target="_blank" rel="noopener">github</a></p></li>
<li><p>Natural Language Processing with Transformers <a href="https://www.oreilly.com/library/view/natural-language-processing/9781098103231/" target="_blank" rel="noopener">the book</a></p></li>
<li><p>Attention is all you need: <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">papier</a></p></li>
<li><p>The annoted transformer, par Guillaume Klein et al, <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">ici</a></p></li>
<li><p>The Illustrated Transformer par Jay Alammar <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">ici</a></p></li>
<li><p><a href="https://medium.com/nerd-for-tech/nlp-theory-and-code-encoder-decoder-models-part-11-30-e686bcb61dc7" target="_blank" rel="noopener">NLP Theory and Code: Encoder-Decoder Models</a></p></li>
</ul>
</section>

<div class="section ablog__blog_comments">
  
  


<div class="section ablog__prev-next">
  <span class="ablog__prev">
    
    
    <a href="bert-classification-for-research-papers.html">
      
      <i class="fa fa-arrow-circle-left"></i>
      
      <span>Bert Classification For Research Papers</span>
    </a>
    
  </span>
  <span class="ablog__spacer">&nbsp;</span>
  <span class="ablog__next">
    
  </span>
</div>

  
  
  <div class="section ablog__comments">
    <h2>Comments</h2>
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname = "disqus_qfyj9nwrcT";
      var disqus_identifier = "/posts/2022/the-transformer-architecture-pytorch/";
      var disqus_title = "Architecture du transformer et implémentation avec Pytorch (Partie I)";
      var disqus_url = "https://godwinh19.github.io/posts/2022/the-transformer-architecture-pytorch";

      (function () {
        var dsq = document.createElement("script");
        dsq.type = "text/javascript";
        dsq.async = true;
        dsq.src = "//" + disqus_shortname + ".disqus.com/embed.js";
        (
          document.getElementsByTagName("head")[0] ||
          document.getElementsByTagName("body")[0]
        ).appendChild(dsq);
      })();
    </script>
    <noscript>
      Please enable JavaScript to view the
      <a href="https://disqus.com/?ref_noscript">
        comments powered by Disqus.</a ></noscript >
    <a href="https://disqus.com" class="dsq-brlink">
      comments powered by <span class="logo-disqus">Disqus</span>
    </a >
  </div>
  
</div>

                </article>
              
              
              
              
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Architecture du transformer et implémentation avec Pytorch (Partie I)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-ou-auto-attention">Self-Attention ou Auto-Attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention-attention-a-tetes-multiples">Multi-head attention: attention à têtes multiples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-feed-forward-layer-ou-couche-a-propagation-avant">The Feed-Forward Layer ou couche à propagation avant</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalisation-ou-normalisation-de-couche">Layer Normalisation ou normalisation de couche</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-embeddings-ou-encastrement-positionnels">Positional embeddings ou encastrement positionnels</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#endnote">Endnote</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#ressources">Ressources</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p style="text-align:center; font-size:0.9rem; color:#666;">
  © Copyright 2022, Godwin H.
  <br>
  Powered by <a href="https://www.sphinx-doc.org/" target="_blank" rel="noopener">Sphinx</a>
  and the <a href="https://pydata-sphinx-theme.readthedocs.io/" target="_blank" rel="noopener">PyData Sphinx Theme</a>.
  <br>
  <a href="https://github.com/godwinh19" target="_blank">GitHub</a> ·
  <a href="/about">About</a> ·
  <a href="/blog">Blog</a>
</p>

<!--2022 Godwin Houdji — Built with ❤️, data, and too much ☕--></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>